<h1 style="text-align: center;font-size: 50px; font-weight: bold;">Projet Big Data</h1>

<h3 style="text-align: center; font-size: 20px; font-weight: bold;">-- by Nicolas Dimitriu and Xavier Moissinac --</h3>
<br><br>



For this project, a data pipeline will be set up with Apache Spark for real-time processing, HDFS for data storage, Hive for analysis, and Apache Kafka for data import. Using Python to create simulated data, the objective is to develop an effective system that gathers, saves, analyzes, and processes sensor data. Numerous technical difficulties could come up when working on the project, particularly when integrating diverse parts and making sure they function together seamlessly. It is essential to comprehend these possible problems and how to resolve them in order to guarantee project success and efficient workflow. 

### **Simplified Architecture with Kafka**

1. **Data Ingestion**:
    - Use **Apache Kafka** to collect data generated by Python and send it to HDFS via a Kafka consumer.
2. **Data Storage**:
    - The data is stored in **HDFS** (in raw format or JSON/CSV).
3. **Data Analysis**:
    - Use **Hive** to analyze the data and detect anomalies.
4. **Data Processing**:
    - Use **Apache Spark** to process and analyze the data in real time, if necessary (replacing MapReduce).

---

### **Project Steps**

### **1. Data Generation with Python**

Generate simulated logs in JSON/CSV format using Python and send them to a Kafka topic.

```
python

from kafka import KafkaProducer
import json
import random
import time

# Fonction to generate simulated datas
def generate_data():
    return {
        "temperature": round(random.uniform(20.0, 30.0), 2),
        "pressure": round(random.uniform(1012.0, 1020.0), 2),
        "timestamp": int(time.time())
    }

# Initialize the producter Kafka with the good port
producer = KafkaProducer(bootstrap_servers=['localhost:9093'], value_serializer=lambda v: json.dumps(v).encode('utf-8'))

# Generate and send datas to a topic Kafka
for _ in range(1000):
    data = generate_data()
    producer.send('sensor_data', value=data)
    time.sleep(1)

# close kafka producer
producer.close()
```


### **2. Data Ingestion with Kafka**

1. **Producteur Kafka** : The above Python script sends data to the **sensor_data** Kafka topic.
2. **Consommateur Kafka** : Use a Kafka consumer to read this data and send it to HDFS.

Here’s an example of a Kafka consumer that reads data from the topic and writes it to HDFS.

```
python

from kafka import KafkaConsumer
from hdfs import InsecureClient
import json

# Initialize the Kafka consumer with the correct address
consumer = KafkaConsumer(
    'sensor_data',
    bootstrap_servers=['localhost:9093'], 
    group_id='sensor_group',
    value_deserializer=lambda x: json.loads(x.decode('utf-8'))
)



# Initialize HDFS client with correct address
hdfs_client = InsecureClient('http://172.17.0.2:50070', user='hadoop')


# Consuming Kafka messages and storing them in HDFS
for message in consumer:
    data = message.value
    try:
        # Add data to JSON file in append mode
        hdfs_client.write('/user/iot/sensor_data/sensor_data.json', data=json.dumps(data) + "\n", append=True)
    except Exception as e:
        print(f"Error writing to HDFS: {e}")
```


### **3. Storing Data in HDFS**

#### 1. Connect to the Hadoop Container

Run the following command to access the Hadoop container:

If you encounter any bugs or issues similar to the ones we faced, I recommend checking the ProblemsAndSolutions.md page.

```bash
docker exec -it hadoop bash
```

#### 2. Create an Empty File in HDFS

Use this command to create an empty file in HDFS:

```bash
hdfs dfs -touchz /user/iot/sensor_data/sensor_data.json
```

#### 3. Verify the File Exists

List the files in the HDFS directory to ensure the file has been created:

```bash
hdfs dfs -ls /user/iot/sensor_data/
```

#### 4. Read the File's Content

Check the content of the file (it will be empty at this stage) with this command:

```bash
hdfs dfs -cat /user/iot/sensor_data/sensor_data.json
```


### **4. Data Analysis with Hive**

Create a Hive table to query the data.

### Create Hive Table

```
sql

CREATE EXTERNAL TABLE sensor_data (
    temperature FLOAT,
    pressure FLOAT,
    timestamp BIGINT
)
STORED AS TEXTFILE
LOCATION '/user/iot/sensor_data/';

```

### Hive Query to Detect Anomalies

```
sql

SELECT *
FROM sensor_data
WHERE temperature > 28 OR pressure < 1015;
```


### **5. Data Processing with Spark**

If you want to perform real-time or more advanced processing on the data, you can integrate **Apache Spark**.

Here’s an example of a Spark script to detect anomalies in real time:

```
python

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialize Spark
spark = SparkSession.builder.appName("SensorDataAnalysis").getOrCreate()

# Load data from HDFS
sensor_data_df = spark.read.json("hdfs://localhost:9000/user/iot/sensor_data/sensor_data.json")

# Detect anomalies (temperature > 28 or pressure < 1015)
anomalies_df = sensor_data_df.filter((col("temperature") > 28) | (col("pressure") < 1015))

# Display anomalies
anomalies_df.show()
```


### **Tools Summary**

| Étape | Outil |
| --- | --- |
| Data Generation |Python |
| Data Ingestion | Apache Kafka |
| Data Storage | Hadoop HDFS |
| Data Analysis | Hive |
| Real-time Processing | Apache Spark |

---

By simplifying in this way, you retain the basic architecture with Kafka for data ingestion and HDFS for storage, while simplifying data processing with Hive and the option to add Spark if needed. This reduces project complexity while keeping a flexible architecture.
