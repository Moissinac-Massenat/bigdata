<h1 style="text-align: center;font-size: 50px; font-weight: bold;">Projet Big Data</h1>

<h3 style="text-align: center; font-size: 20px; font-weight: bold;">-- by Nicolas Dimitriu and Xavier Moissinac --</h3>
<br><br>



For this project, a data pipeline will be set up with Apache Spark for real-time processing, HDFS for data storage, Hive for analysis, and Apache Kafka for data import. Using Python to create simulated data, the objective is to develop an effective system that gathers, saves, analyzes, and processes sensor data. Numerous technical difficulties could come up when working on the project, particularly when integrating diverse parts and making sure they function together seamlessly. It is essential to comprehend these possible problems and how to resolve them in order to guarantee project success and efficient workflow. 
<br><br>
Before starting the next section, go to the ```config.md``` to prepare your environment.

### **Simplified Architecture with Kafka**

1. **Data Ingestion**:
    - Use **Apache Kafka** to collect data generated by Python and send it to HDFS via a Kafka consumer.
2. **Data Storage**:
    - The data is stored in **HDFS** (in raw format or JSON/CSV).
3. **Data Analysis**:
    - Use **Hive** to analyze the data and detect anomalies.
4. **Data Processing**:
    - Use **Apache Spark** to process and analyze the data in real time, if necessary (replacing MapReduce).

---

### **Project Steps**

### **1. Data Generation with Python**

Generate simulated logs in JSON/CSV format using Python and send them to a Kafka topic.

```
python

from kafka import KafkaProducer
import json
import random
import time

# Fonction pour générer des données simulées
def generate_data():
    return {
        "sensor_id": random.randint(1, 10),  # ID du capteur, pour varier
        "temperature": round(random.uniform(20.0, 30.0), 2),
        "pressure": round(random.uniform(1012.0, 1020.0), 2),
        "humidity": round(random.uniform(30.0, 70.0), 2),  # Ajout d'un champ d'humidité
        "timestamp": int(time.time())
    }

# Fonction de callback pour afficher si le message a été envoyé correctement
def on_send_success(record_metadata):
    print(f"Message envoyé avec succès à {record_metadata.topic} partition {record_metadata.partition} à l'offset {record_metadata.offset}")

def on_send_error(excp):
    print(f"Erreur d'envoi du message : {excp}")

# Initialiser le producteur Kafka avec le bon port
producer = KafkaProducer(
    bootstrap_servers=['localhost:9093'],  # Assurez-vous que ce port est correct
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
   acks='all',  # Assurez-vous que Kafka attend la confirmation de tous les réplicas
    retries=3  # Réessayez 3 fois en cas d'échec
)

# Générer et envoyer les données vers un topic Kafka
for _ in range(1000):
    data = generate_data()
    producer.send('sensor_data', value=data).add_callback(on_send_success).add_errback(on_send_error)
    print(f"Sent data: {data}")  # Afficher ce qui est envoyé
    time.sleep(1)

# Attendre que tous les messages soient envoyés avant de fermer le producteur
producer.flush()

# Fermer le producteur Kafka
producer.close()

```


### **2. Data Ingestion with Kafka**

1. **Producteur Kafka** : The above Python script sends data to the **sensor_data** Kafka topic.
2. **Consommateur Kafka** : Use a Kafka consumer to read this data and send it to HDFS.

Here’s an example of a Kafka consumer that reads data from the topic and writes it to HDFS.

```
python

from kafka import KafkaConsumer
from hdfs import InsecureClient
import json

# Consommateur Kafka
consumer = KafkaConsumer(
    'sensor_data',
    bootstrap_servers=['localhost:9093'],  # Utilise l'IP du conteneur Kafka
    group_id='sensor_group',
    value_deserializer=lambda x: json.loads(x.decode('utf-8')),
    auto_offset_reset='earliest',
    enable_auto_commit=True,
)

# Initialiser le client HDFS avec l'adresse correcte du conteneur Hadoop
hdfs_client = InsecureClient('http://172.19.0.6:50070', user='hadoop')  # Utiliser 50070 ici pour l'accès WebHDFS

# Consommer les messages de Kafka et les stocker dans HDFS
for message in consumer:
    data = message.value
    try:
        # Afficher les données reçues
        print(f"Received data: {data}")

        # Vérifier si le fichier existe dans HDFS avant d'écrire
        file_path = '/user/iot/sensor_data/sensor_data.json'
        try:
            hdfs_client.status(file_path)  # Vérifie si le fichier existe
            file_exists = True
        except FileNotFoundError:
            file_exists = False
        
        # Si le fichier n'existe pas, crée-le et écrit les données
        if not file_exists:
            with hdfs_client.write(file_path, overwrite=True) as writer:
                writer.write(json.dumps(data) + "\n")
        else:
            # Ajouter les données dans le fichier JSON en mode append
            with hdfs_client.write(file_path, append=True) as writer:
                writer.write(json.dumps(data) + "\n")
        
        # Confirmer l'ajout des données dans la console
        print("Data written to HDFS successfully.")
    except Exception as e:
        # Afficher l'erreur en cas de problème avec HDFS
        print(f"Error writing to HDFS: {e}")

```


### **3. Storing Data in HDFS**

#### 1. Connect to the Hadoop Container

Run the following command to access the Hadoop container:

If you encounter any bugs or issues similar to the ones we faced, I recommend checking the ```ProblemsAndSolutions.md``` page.

```bash
docker exec -it hadoop bash
```

#### 2. Create an Empty File in HDFS

Use this command to create an empty file in HDFS:

```bash
hdfs dfs -touchz /user/iot/sensor_data/sensor_data.json
```

#### 3. Verify the File Exists

List the files in the HDFS directory to ensure the file has been created:

```bash
hdfs dfs -ls /user/iot/sensor_data/
```

#### 4. Read the File's Content

Check the content of the file (it will be empty at this stage) with this command:

```bash
hdfs dfs -cat /user/iot/sensor_data/sensor_data.json
```

After following the steps to create and verify the file in Hadoop's HDFS, you can now see all the data generated in the system.

### **4. Data Analysis with Hive**

Create a Hive table to query the data.

### Create Hive Table

```
sql

CREATE EXTERNAL TABLE sensor_data (
    temperature FLOAT,
    pressure FLOAT,
    timestamp BIGINT
)
STORED AS TEXTFILE
LOCATION '/user/iot/sensor_data/';

```

### Hive Query to Detect Anomalies

```
sql

SELECT *
FROM sensor_data
WHERE temperature > 28 OR pressure < 1015;
```


### **5. Data Processing with Spark**

If you want to perform real-time or more advanced processing on the data, you can integrate **Apache Spark**.

Here’s an example of a Spark script to detect anomalies in real time:

```
python

from pyspark.sql import SparkSession
from pyspark.sql.functions import col

# Initialiser Spark
spark = SparkSession.builder.appName("SensorDataAnalysis").getOrCreate()

# Charger les données depuis HDFS
sensor_data_df = spark.read.json("hdfs://172.17.0.2:9000/user/iot/sensor_data/sensor_data.json")

# Détecter les anomalies (température > 28 ou pression < 1015)
anomalies_df = sensor_data_df.filter((col("temperature") > 28) | (col("pressure") < 1015))

# Afficher les anomalies
anomalies_df.show()
```


### **Tools Summary**

| Étape | Outil |
| --- | --- |
| Data Generation |Python |
| Data Ingestion | Apache Kafka |
| Data Storage | Hadoop HDFS |
| Data Analysis | Hive |
| Real-time Processing | Apache Spark |

---

By simplifying in this way, you retain the basic architecture with Kafka for data ingestion and HDFS for storage, while simplifying data processing with Hive and the option to add Spark if needed. This reduces project complexity while keeping a flexible architecture.

## 6. Challenges Faced in the Big Data Project

The integration of multiple technologies into a real-time data pipeline presented a number of technical difficulties during this project. Below is a synopsis of the primary challenges and the solutions offered:

1. **Docker Container Configuration Issue**:  
   Communication between the Docker services (Kafka, HDFS, Hive, Spark) was difficult to establish. The solution was to properly configure Docker networks and ensure that the IP addresses and ports were accessible between the containers.

2. **Hive Configuration with HDFS**:  
   Hive could not connect correctly to HDFS. We resolved this by configuring the environment variables **$HADOOP_HOME** and **$HADOOP_PREFIX** in the Hive container, allowing Hive to access Hadoop and analyze the data.

3. **Writing Data from Kafka to HDFS Issue**:  
   Errors occurred when Kafka attempted to send data to HDFS. After adjusting the HDFS connection URL and verifying write permissions, we solved the issue by adding error handling mechanisms.

4. **Not having access to SSH was a real problem**
